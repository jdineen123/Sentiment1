{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework 2\n",
    "#### Scripting for Data Analysis\n",
    "#### 8/26/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, I will be using jupyter notebooks to perform my analysis, as well as my writeup. I find the structure of having all written exploration and all code in one place helps to digest some material, as well as allowing for rapid prototyping and quick changes.\n",
    "\n",
    "\n",
    "The goal of this assignment is to read in some form of JSON data, whether that be from an API like tweepy, or straight from a mongodb. I'd like to consolidate the two, just to get some more practice using a document store like pymongo as well as a social media API, which I have worked with in the past, but know have a bit more practice dissecting lists of dictionaries. I also might explore using a pretrained sentiment analysis model (NLTK) to show some notion of a distribution between positive and negative sentiment over time - I've found this to be a sometimes viable source for mass labeling datasets, although there is a bit of a negative bias when using NLTK (bias is unavoidable when dealing with human annotation tasks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a mongodb connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:18:20.378162Z",
     "start_time": "2018-08-27T05:18:20.346248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local', 'usgs']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)  #Set up client\n",
    "\n",
    "#b.create_collection('tweets')\n",
    "db.list_collection_names()\n",
    "client.list_database_names()\n",
    "client.drop_database('twitter_storage')\n",
    "client.list_database_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:18:21.938988Z",
     "start_time": "2018-08-27T05:18:21.908071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: |twitter_storage| is ready @ 2018-08-26 22:18:21\n",
      "-------------------------\n",
      "DBcollection: |tweets| is ready @ 2018-08-26 22:18:21\n",
      "-------------------------\n",
      "Time taken to execute: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import localtime, strftime\n",
    "import timeit\n",
    "import time\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import warnings\n",
    "\n",
    "client = MongoClient('localhost', 27017)  #Set up client\n",
    "\n",
    "\n",
    "def create_db(database, collection_name):\n",
    "    start = time.time()\n",
    "    #wasn't sure if this just rewrote over it. I don't believe it does, but that's the rationale for the try-catch\n",
    "    #borrowed some of the below from the db_fn notebook. Seems like an easier way of handling type sensitivity to just convert after input\n",
    "    dbname = name.lower()\n",
    "    dbname = dbname.replace('#', '')\n",
    "    dbname = dbname.replace(' ', '')\n",
    "    dbCollection = collect.lower()\n",
    "    dbCollection = dbCollection.replace('#', '')\n",
    "    dbCollection = dbCollection.replace(' ', '')\n",
    "    # use database named usgs or create it if not there already\n",
    "    db = client[dbname]\n",
    "    # create collection\n",
    "    collection = db[dbCollection]\n",
    "    end = time.time()\n",
    "    #Logs\n",
    "    print('DB: |{}| is ready @ {}'.format(\n",
    "        dbname, strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "    print('-------------------------')\n",
    "    print('DBcollection: |{}| is ready @ {}'.format(\n",
    "        dbCollection, strftime(\"%Y-%m-%d %H:%M:%S\", localtime())))\n",
    "    print('-------------------------')\n",
    "    print('Time taken to execute: {} seconds'.format(end - start))\n",
    "\n",
    "\n",
    "#instantiate\n",
    "name = 'twitter_storage'\n",
    "collect = 'tweets'\n",
    "\n",
    "create_db(name, collect)\n",
    "\n",
    "#Testing upload\n",
    "#create_db(name, collect)\n",
    "#collection.insert_one({'j': 1})\n",
    "#client.list_database_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-26T23:13:33.768614Z",
     "start_time": "2018-08-26T23:13:33.765623Z"
    }
   },
   "source": [
    "#### Starting with twython\n",
    "The first step here will be to collect some data from tweepy. I had luckily set up a twitter dev account in a previous class, so I'm taking the keys from that notebook. I'm trying to do focus on functional programming over scripting, so I'm breaking this into some main functions that I want to call. This could then be easily transferred to a .py file and ran.\n",
    "\n",
    "As far as the actual query, I settled on Donald Trump. I know this is a pretty common project since he's very active on twitter, but I have never explored things like mass-sentiment analysis of an individual based on tweets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:18:58.426418Z",
     "start_time": "2018-08-27T05:18:25.228696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully scraped and pulled 5051 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1f7c502abc8>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "#Likely want to import this in from a separate .py file where possible. Didn't think it was highly sensitive, so left it in here.\n",
    "consumer_key = 'K3qj35aGWoALJaBdqSUgszqoc'\n",
    "consumer_secret = 'hyBrGQXXbyTah2btx29di8w92SlgFN9fu48s0KjCJKJpiZI4Zx'\n",
    "access_token = '1019690305237311488-b04ol1OxAOznie1ir1wWlneoetChYx'\n",
    "access_token_secret = 'QPc4KDfNraTT5S63kcvu5t8GTL1H5y5MkyE9EZCupvUQD'\n",
    "\n",
    "q = 'Donald Trump'\n",
    "\n",
    "\n",
    "def fetch_tweets(query,\n",
    "                 consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token=access_token,\n",
    "                 access_token_secret=access_token_secret):\n",
    "    '''\n",
    "    query: string\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth)\n",
    "        placeholder = []\n",
    "\n",
    "        #This is the cleanest way I could think of doing this.\n",
    "        #I'm basically trying to bypass the 100 tweet limit by continuing w/ a nested loop until a threshold is reached.\n",
    "        for i in api.search(q=query, lang=\"en\", count=100):\n",
    "            placeholder.append(\n",
    "                i._json\n",
    "            )  #transforming to json and storing in a list named placeholder\n",
    "            counter = len(placeholder)\n",
    "            if counter > 5000:\n",
    "                break\n",
    "            else:\n",
    "                for i in api.search(q=query, lang=\"en\", count=100):\n",
    "                    placeholder.append(\n",
    "                        i._json\n",
    "                    )  #transforming to json and storing in a list named placeholder\n",
    "                    continue\n",
    "\n",
    "        print('You have successfully scraped and pulled {} tweets'.format(\n",
    "            len(placeholder)))\n",
    "        return placeholder\n",
    "    except:\n",
    "        print('Unable to scrape tweets')\n",
    "\n",
    "\n",
    "tweets = fetch_tweets(\n",
    "    query='Donald Trump')  #Save an instance with the json data\n",
    "\n",
    "collection.insert_many(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here, I thought it might be interesting to look at the distribution of my 5000+ tweets by location.\n",
    "\n",
    "This is a bit wonky, but it's definitely a learning experience. It's important to note that the 5k tweets that I pulled came in with a range of a matter of seconds, so showing any kind of time series distribution would be at the 'second' level. That's why I'm avoiding that here. Instead, looking at a distribution of tweet count by region, I can begin to see a sample of the total population. We also start to notice some of the flaws with manual entry form submission for application access - The variation in the manner of responses is inherently flawed. United States and USA are listed as two different Geos. 'God Bless Our Veterans' is listed as a geo in its own regard - That makes me think that people are becoming increasingly aware of the abilities for scrapers and humans alike to form a profile based on their web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:23:51.933120Z",
     "start_time": "2018-08-27T05:23:51.905196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>geos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Carolina, USA</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California, USA</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>European Union 🇪🇺🇩🇪🇨🇮</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index  geos\n",
       "0                      USA   172\n",
       "1            United States   170\n",
       "2             Florida, USA    90\n",
       "3      North Carolina, USA    80\n",
       "4            Nashville, TN    72\n",
       "5          California, USA    71\n",
       "6    European Union 🇪🇺🇩🇪🇨🇮    63\n",
       "7  Sydney, New South Wales    57\n",
       "8             Tokyo, Japan    51\n",
       "9        San Francisco, CA    51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets[0]\n",
    "\n",
    "times = []\n",
    "geos = []\n",
    "text = []\n",
    "\n",
    "from collections import Counter\n",
    "#Store data in lists\n",
    "for i in tweets:\n",
    "    times.append(i['created_at'])\n",
    "    geos.append(i['user']['location'])\n",
    "    text.append(i['text'])\n",
    "\n",
    "import pandas as pd\n",
    "#Convert lists to a pandas dataframe for grouping\n",
    "df = pd.DataFrame()\n",
    "df['geos'] = geos\n",
    "df['time'] = times\n",
    "df['text'] = text\n",
    "\n",
    "#Group by count of geos\n",
    "df_geos = df['geos'].groupby(by=geos).count()[1:]\n",
    "df_geos = pd.DataFrame(df_geos.sort_values(ascending=False)[:10])\n",
    "df_geos.reset_index(inplace=True)\n",
    "display(df_geos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis\n",
    "Next, I'd like to show something like the above but with the added twist of some kind of scoring mechanism to show sentiment of tweets by region of the fixed period of time. I don't necessarily want to build my own classifier here, so I'll use a neural net with pretrained weights based off of fitting on social media text. \n",
    "\n",
    "- Admittedly, it would have been better for me to do some kind of string match against a list of viable geos across the world. As I moved through this assignment, it made it very difficult to actual do analysis because most users don't enter realistic locations into apps/websites when signing up for service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:19:28.173863Z",
     "start_time": "2018-08-27T05:19:27.040893Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#prime sentiment analyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#def print_sentiment_scores(sentence):\n",
    "#    snt = analyser.polarity_scores(sentence)\n",
    "#    print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "\n",
    "\n",
    "#Append dict scores to a list\n",
    "empty = []\n",
    "for i in text:\n",
    "    empty.append(analyser.polarity_scores(i))\n",
    "\n",
    "#convert list of dicts to dataframe\n",
    "polarity = pd.DataFrame(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:19:36.675127Z",
     "start_time": "2018-08-27T05:19:36.649197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 5 geos, sorted by negative_score \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>21.051</td>\n",
       "      <td>133.523</td>\n",
       "      <td>17.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Antonio TX</th>\n",
       "      <td>20.727</td>\n",
       "      <td>18.963</td>\n",
       "      <td>9.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nashville, TN</th>\n",
       "      <td>20.613</td>\n",
       "      <td>46.287</td>\n",
       "      <td>5.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>European Union 🇪🇺🇩🇪🇨🇮</th>\n",
       "      <td>18.235</td>\n",
       "      <td>40.065</td>\n",
       "      <td>4.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>17.748</td>\n",
       "      <td>29.070</td>\n",
       "      <td>4.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       negative_score  neutral_score  positive_score\n",
       "geos                                                                \n",
       "USA                            21.051        133.523          17.375\n",
       "San Antonio TX                 20.727         18.963           9.310\n",
       "Nashville, TN                  20.613         46.287           5.100\n",
       "European Union 🇪🇺🇩🇪🇨🇮          18.235         40.065           4.700\n",
       "Germany                        17.748         29.070           4.182"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 5 geos, sorted by neutral_score \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>12.079</td>\n",
       "      <td>145.783</td>\n",
       "      <td>12.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>21.051</td>\n",
       "      <td>133.523</td>\n",
       "      <td>17.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida, USA</th>\n",
       "      <td>8.774</td>\n",
       "      <td>81.226</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina, USA</th>\n",
       "      <td>4.324</td>\n",
       "      <td>73.058</td>\n",
       "      <td>2.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California, USA</th>\n",
       "      <td>5.180</td>\n",
       "      <td>65.820</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     negative_score  neutral_score  positive_score\n",
       "geos                                                              \n",
       "United States                12.079        145.783          12.150\n",
       "USA                          21.051        133.523          17.375\n",
       "Florida, USA                  8.774         81.226           0.000\n",
       "North Carolina, USA           4.324         73.058           2.618\n",
       "California, USA               5.180         65.820           0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 5 geos, sorted by positive_score \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>21.051</td>\n",
       "      <td>133.523</td>\n",
       "      <td>17.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tokyo, Japan</th>\n",
       "      <td>5.610</td>\n",
       "      <td>30.141</td>\n",
       "      <td>15.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States</th>\n",
       "      <td>12.079</td>\n",
       "      <td>145.783</td>\n",
       "      <td>12.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wherever Music Is Playing</th>\n",
       "      <td>8.040</td>\n",
       "      <td>20.360</td>\n",
       "      <td>11.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spokane Valley, WA (STATE)</th>\n",
       "      <td>0.000</td>\n",
       "      <td>39.474</td>\n",
       "      <td>11.526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             negative_score  neutral_score  positive_score\n",
       "geos                                                                      \n",
       "USA                                  21.051        133.523          17.375\n",
       "Tokyo, Japan                          5.610         30.141          15.249\n",
       "United States                        12.079        145.783          12.150\n",
       "Wherever Music Is Playing             8.040         20.360          11.600\n",
       "Spokane Valley, WA (STATE)            0.000         39.474          11.526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Add the columns from the polarity df to the original df with the geo data\n",
    "df['negative_score'] = polarity['neg']\n",
    "df['neutral_score'] = polarity['neu']\n",
    "df['positive_score'] = polarity['pos']\n",
    "\n",
    "#Group by geos + score\n",
    "import numpy as np\n",
    "sent_geo = df.groupby(\n",
    "    ['geos'])[['negative_score', 'neutral_score', 'positive_score']].sum()[1:]\n",
    "\n",
    "#Iterate through a list to display sorting by different scores\n",
    "score_types = ['negative_score', 'neutral_score', 'positive_score']\n",
    "for i in score_types:\n",
    "    print('Displaying the top 5 geos, sorted by {} '.format(i))\n",
    "    display(sent_geo.sort_values(by=i, ascending=False)[:5])\n",
    "\n",
    "#sent_geo.sort_values(by = 'negative_score', ascending= False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T05:21:00.920825Z",
     "start_time": "2018-08-27T05:21:00.851011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geos</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana, USA</td>\n",
       "      <td>Mon Aug 27 05:18:09 +0000 2018</td>\n",
       "      <td>@FaceLess1930 @DineshDSouza i present the man ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geos                            time  \\\n",
       "0  Indiana, USA  Mon Aug 27 05:18:09 +0000 2018   \n",
       "\n",
       "                                                text  negative_score  \\\n",
       "0  @FaceLess1930 @DineshDSouza i present the man ...             0.0   \n",
       "\n",
       "   neutral_score  positive_score  \n",
       "0            1.0             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geos</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana, USA</td>\n",
       "      <td>Mon Aug 27 05:18:09 +0000 2018</td>\n",
       "      <td>@FaceLess1930 @DineshDSouza i present the man ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Mon Aug 27 05:18:09 +0000 2018</td>\n",
       "      <td>Obama was the worst President in History!\\nPre...</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.200</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indiana, USA</td>\n",
       "      <td>Mon Aug 27 05:18:09 +0000 2018</td>\n",
       "      <td>@FaceLess1930 @DineshDSouza i present the man ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Mon Aug 27 05:18:08 +0000 2018</td>\n",
       "      <td>RT @DFBHarvard: You realize there is only ONE ...</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>Mon Aug 27 05:18:08 +0000 2018</td>\n",
       "      <td>Fox News Host Slams Dotard Don: ‘What Good Is ...</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.125</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geos                            time  \\\n",
       "0   Indiana, USA  Mon Aug 27 05:18:09 +0000 2018   \n",
       "1                 Mon Aug 27 05:18:09 +0000 2018   \n",
       "2   Indiana, USA  Mon Aug 27 05:18:09 +0000 2018   \n",
       "3                 Mon Aug 27 05:18:08 +0000 2018   \n",
       "4  United States  Mon Aug 27 05:18:08 +0000 2018   \n",
       "\n",
       "                                                text  negative_score  \\\n",
       "0  @FaceLess1930 @DineshDSouza i present the man ...           0.000   \n",
       "1  Obama was the worst President in History!\\nPre...           0.148   \n",
       "2  @FaceLess1930 @DineshDSouza i present the man ...           0.000   \n",
       "3  RT @DFBHarvard: You realize there is only ONE ...           0.214   \n",
       "4  Fox News Host Slams Dotard Don: ‘What Good Is ...           0.099   \n",
       "\n",
       "   neutral_score  positive_score sentiment  \n",
       "0          1.000           0.000   neutral  \n",
       "1          0.652           0.200   neutral  \n",
       "2          1.000           0.000   neutral  \n",
       "3          0.786           0.000   neutral  \n",
       "4          0.776           0.125   neutral  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'negative': 246, 'neutral': 4748, 'positive': 57})"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "display(df.head(1))\n",
    "\n",
    "#Conditional Logic to create new var based on treshold\n",
    "df['sentiment'] = np.where(df['negative_score'] >= 0.33, 'negative',\n",
    "                          np.where(df['positive_score'] >= 0.33, 'positive',\n",
    "                                  np.where(df['neutral_score'] >= 0.33, 'neutral', 0)))\n",
    "\n",
    "display(df.head(5))\n",
    "\n",
    "#Plotting sentiment count by cat var\n",
    "sns.countplot(x = 'sentiment', data = df)\n",
    "plt.show()\n",
    "from collections import Counter\n",
    "Counter(df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In conclusion\n",
    "\n",
    "This was a very interesting, fun project. I spent the better part of two days researching and writing the code that led to this point. I also had some serious debugging issues that set me back, but ultimately helped me troubleshoot what was going on in my code. In terms of the actual results, they were very surprising, regarding the sentiment analysis portion of the task. I assumed that most of the tweets that were scraped would have a negative connotation, and it is possible that the Saber pretrained NN has some inherent bias towards neutral class mapping, but still, having over 90% of the tweets fall under that class is something. It should be noted that these tweets were pulled within about a one minute span of one another, so they aren't distributed over a number of days. To do that I imagine we could just run a daily job, similar to this, and store all of the documents in our mongodb (That's kind of why I included that part, as well as just refamiliarizing myself with the API). "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
